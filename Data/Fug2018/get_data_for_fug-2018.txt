1. Download the original data [1] into this fold. The downloaded files in this fold should contains: AUDIO.zip, DATA_preproc.zip, EEG.zip.
2. Unzip AUDIO.zip, DATA_preproc.zip and EEG.zip to this fold.
3. Run get_expinfo_preprocess.m to remove the info of the single-speaker trials.
3. Run preprocess_data_raw.m and you will obtain a folder named "preprocessed", which contains the extracted EEG and corresponding label (./preprocessed/S*_raw.mat).
4. Run add_wavelet.m to conduct filtering, re-reference and wavelet transform. The output file named "S*_preprocessed.mat" are stored in the preprocessed folder (./preprocessed/S*_preprocessed.mat).
5. Run get_data_python.py to convert data format. The output file named "S*_preprocessed_py.mat" are stored in the preprocessed folder (./preprocessed/S*_preprocessed_py.mat).

Note: 
1. The preprocess_data_raw.m is modified from /src/examples/preproc_data.m in https://zenodo.org/records/1199011#.Yk6evNtBz_U.
2. For direction classification, the label 0 and 1 represent left and right, respectively.

















-------------------------------
References:
[1] Søren A. Fuglsang, Torsten Dau & Jens Hjortkjær (2017): Noise-robust cortical tracking of attended speech in real-life environments. NeuroImage, 156, 435-444